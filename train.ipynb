{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUhPTUdw-Fbm"
      },
      "source": [
        "# MCOC Action Predictor from Video Frames\n",
        "\n",
        "This notebook trains a deep learning model to predict actions in the game *Marvel Contest of Champions (MCOC)* based on sequences of screen frames. The model uses a pre-trained MobileNetV2 as a CNN backbone to extract features from each frame, followed by a GRU network to understand the temporal sequence of these features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2EE9pMc-Fbp"
      },
      "source": [
        "### 1. Imports\n",
        "\n",
        "First, let's import all the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpg6KUnC-Fbq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from typing import List, Tuple, Dict\n",
        "import json\n",
        "import sys\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLLvd3wf-Fbt"
      },
      "source": [
        "### 2. Configuration\n",
        "\n",
        "Instead of importing from a `config.py` file, we define our configuration parameters directly in the notebook for better portability. This includes paths, model hyperparameters, and data settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqae2K8--Fbu"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # Directories\n",
        "    DATA_DIR = 'assets/data/raw' # <--- MODIFIED PATH\n",
        "    MODEL_SAVE_DIR = 'models/'\n",
        "    RESULTS_DIR = 'results/'\n",
        "\n",
        "    # Data Parameters\n",
        "    IMAGE_SIZE = 128\n",
        "    SEQUENCE_LENGTH = 10\n",
        "    LABEL_MAPPING = {\n",
        "        'light_attack': 0,\n",
        "        'medium_attack': 1,\n",
        "        'heavy_attack': 2,\n",
        "        'special_attack': 3,\n",
        "        'idle': 4\n",
        "    }\n",
        "    NUM_CLASSES = len(LABEL_MAPPING)\n",
        "\n",
        "    # Model Hyperparameters\n",
        "    FEATURE_DIM = 256\n",
        "    HIDDEN_SIZE = 128\n",
        "    DROPOUT_RATE = 0.3\n",
        "\n",
        "    # Training Parameters\n",
        "    BATCH_SIZE = 16\n",
        "    LEARNING_RATE = 1e-4\n",
        "    NUM_EPOCHS = 10\n",
        "\n",
        "    # Convert class attributes to a dictionary for saving\n",
        "    @classmethod\n",
        "    def __dict__(cls):\n",
        "        return {k: v for k, v in cls.__dict__.items() if not k.startswith('__') and not callable(v)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAi8I7tT-Fbz"
      },
      "source": [
        "### 4. Dataset Class\n",
        "\n",
        "The `MCOCDataset` class handles loading the data. It takes a list of file paths, creates overlapping sequences of a specified length (`sequence_length`), and extracts the label from the filename of the last frame in each sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai1eHVWx-Fb0"
      },
      "outputs": [],
      "source": [
        "class MCOCDataset(Dataset):\n",
        "    def __init__(self, file_paths: List[str], sequence_length: int = 10, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.sequence_length = sequence_length\n",
        "        self.transform = transform\n",
        "        self.sequences = self._create_sequences()\n",
        "\n",
        "    def _create_sequences(self) -> List[Tuple[List[str], int]]:\n",
        "        \"\"\"Crea sequenze di frame con le relative label\"\"\"\n",
        "        sequences = []\n",
        "\n",
        "        # Raggruppa i file per sequenze consecutive\n",
        "        for i in range(len(self.file_paths) - self.sequence_length + 1):\n",
        "            sequence_files = self.file_paths[i:i + self.sequence_length]\n",
        "\n",
        "            # Estrai la label dal nome del file (ultimo frame della sequenza)\n",
        "            last_file = sequence_files[-1]\n",
        "            label = self._extract_label(last_file)\n",
        "\n",
        "            if label is not None:\n",
        "                sequences.append((sequence_files, label))\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def _extract_label(self, filename: str) -> int:\n",
        "        \"\"\"Estrae la label dal nome del file\"\"\"\n",
        "        # Estrai la parte dopo l'underscore\n",
        "        match = re.search(r'_([^.]+)\\.png$', filename)\n",
        "        if match:\n",
        "            label_str = match.group(1)\n",
        "            return Config.LABEL_MAPPING.get(label_str)\n",
        "        return None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence_files, label = self.sequences[idx]\n",
        "\n",
        "        # Carica le immagini della sequenza\n",
        "        images = []\n",
        "        for file_path in sequence_files:\n",
        "            img = Image.open(file_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            images.append(img)\n",
        "\n",
        "        # Stack delle immagini in un tensor (T, C, H, W)\n",
        "        images_tensor = torch.stack(images)\n",
        "\n",
        "        return {\n",
        "            'images': images_tensor,\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG4H5_Qg-Fb1"
      },
      "source": [
        "### 5. Model Architecture\n",
        "\n",
        "The `MCOCActionPredictor` is a hybrid CNN-RNN model.\n",
        "- **CNN Backbone**: A pre-trained `MobileNetV2` is used to extract spatial features from each frame. We freeze the initial layers to leverage pre-trained knowledge and reduce computation.\n",
        "- **Feature Extractor**: A fully connected layer reduces the dimensionality of the features extracted by the CNN.\n",
        "- **RNN (GRU)**: A Gated Recurrent Unit (GRU) network processes the sequence of frame features to capture temporal dependencies.\n",
        "- **Classifier**: A final set of linear layers predicts the action class based on the GRU's output from the last timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kut5B6W-Fb2"
      },
      "outputs": [],
      "source": [
        "class MCOCActionPredictor(nn.Module):\n",
        "    def __init__(self, feature_dim=256, hidden_size=128, num_classes=5, dropout_rate=0.3):\n",
        "        super(MCOCActionPredictor, self).__init__()\n",
        "\n",
        "        # CNN Backbone (MobileNetV2 preaddestrato)\n",
        "        self.cnn = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
        "\n",
        "        # Freeze i primi layer per risparmiare memoria\n",
        "        for param in self.cnn.features[:10].parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Rimuovi il classificatore originale di MobileNetV2\n",
        "        self.cnn.classifier = nn.Identity()\n",
        "\n",
        "        # Feature extractor per ottenere feature_dim dimensioni\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1280, feature_dim),  # 1280 Ã¨ l'output di MobileNetV2\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # RNN (GRU) per la sequenza temporale\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=feature_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_rate if 2 > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Classificatore finale\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, channels, height, width)\n",
        "        batch_size, seq_len, c, h, w = x.shape\n",
        "\n",
        "        # Applica CNN a ogni frame della sequenza\n",
        "        cnn_features = []\n",
        "        for t in range(seq_len):\n",
        "            frame = x[:, t, :, :, :]  # (batch_size, c, h, w)\n",
        "            # Prima passa attraverso le features di MobileNetV2\n",
        "            features = self.cnn.features(frame)  # (batch_size, 1280, H, W)\n",
        "            # Poi applica il feature extractor\n",
        "            features = self.feature_extractor(\n",
        "                features)  # (batch_size, feature_dim)\n",
        "            cnn_features.append(features)\n",
        "\n",
        "        # Stack delle feature in sequenza temporale\n",
        "        # (batch_size, seq_len, feature_dim)\n",
        "        sequence_features = torch.stack(cnn_features, dim=1)\n",
        "\n",
        "        # Passa attraverso GRU\n",
        "        # (batch_size, seq_len, hidden_size)\n",
        "        gru_output, _ = self.gru(sequence_features)\n",
        "\n",
        "        # Usa solo l'output dell'ultimo timestep per la predizione\n",
        "        final_output = gru_output[:, -1, :]  # (batch_size, hidden_size)\n",
        "\n",
        "        # Classificazione finale\n",
        "        logits = self.classifier(final_output)  # (batch_size, 5)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-nCAVmY-Fb3"
      },
      "source": [
        "### 6. Training and Validation Functions\n",
        "\n",
        "These helper functions define the logic for a single training epoch and a single validation epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hiFpzNH-Fb4"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        images = batch['images'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total\n",
        "\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images = batch['images'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total, all_predictions, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc4gfGXk-Fb5"
      },
      "source": [
        "### 7. Main Execution: Data Loading and Preparation\n",
        "\n",
        "Here we set up the device, define the image transformations, load the file paths, and split the data into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd74r2BG-Fb5"
      },
      "outputs": [],
      "source": [
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Data preprocessing and augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
        "                         0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load and organize data\n",
        "print(\"Loading data...\")\n",
        "all_files = []\n",
        "for filename in os.listdir(Config.DATA_DIR):\n",
        "    if filename.endswith('.png'):\n",
        "        file_path = os.path.join(Config.DATA_DIR, filename)\n",
        "        all_files.append(file_path)\n",
        "\n",
        "# Sort files by number to maintain temporal order\n",
        "all_files = [f for f in all_files if re.search(\n",
        "    r'\\d+', os.path.basename(f))]\n",
        "\n",
        "# Sort files by the first number found\n",
        "all_files.sort(key=lambda x: int(\n",
        "    re.search(r'\\d+', os.path.basename(x)).group(0)))\n",
        "\n",
        "print(f\"Total files found: {len(all_files)}\")\n",
        "\n",
        "# Split data\n",
        "train_files, test_files = train_test_split(\n",
        "    all_files, test_size=0.2, random_state=42, shuffle=False) # Shuffle=False is important for temporal data\n",
        "train_files, val_files = train_test_split(\n",
        "    train_files, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "print(f\"Train files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "print(f\"Test files: {len(test_files)}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MCOCDataset(train_files, Config.SEQUENCE_LENGTH, transform)\n",
        "val_dataset = MCOCDataset(val_files, Config.SEQUENCE_LENGTH, transform)\n",
        "test_dataset = MCOCDataset(test_files, Config.SEQUENCE_LENGTH, transform)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train sequences: {len(train_dataset)}\")\n",
        "print(f\"Validation sequences: {len(val_dataset)}\")\n",
        "print(f\"Test sequences: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3eJjdri-Fb6"
      },
      "source": [
        "### 8. Model Initialization and Training Loop\n",
        "\n",
        "We initialize the model, loss function (Criterion), optimizer, and a learning rate scheduler. Then, we run the training loop for the specified number of epochs, saving the best-performing model based on validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhlHHWT4-Fb7"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = MCOCActionPredictor(\n",
        "    feature_dim=Config.FEATURE_DIM,\n",
        "    hidden_size=Config.HIDDEN_SIZE,\n",
        "    num_classes=Config.NUM_CLASSES,\n",
        "    dropout_rate=Config.DROPOUT_RATE\n",
        ").to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(), lr=Config.LEARNING_RATE, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting training...\")\n",
        "best_val_acc = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(Config.NUM_EPOCHS):\n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_acc, _, _ = validate_epoch(\n",
        "        model, val_loader, criterion, device)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'config': Config.__dict__()\n",
        "        }, os.path.join(Config.MODEL_SAVE_DIR, 'best_model.pth'))\n",
        "\n",
        "    # Log progress\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{Config.NUM_EPOCHS}]')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybX-NDvy-Fb7"
      },
      "source": [
        "### 9. Plotting Training Curves\n",
        "\n",
        "Visualizing the training and validation loss/accuracy helps in understanding the model's learning progress and diagnosing issues like overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlRPciqP-Fb8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Train Accuracy')\n",
        "plt.plot(val_accs, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(Config.RESULTS_DIR, 'training_curves.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gyaapi8-Fb8"
      },
      "source": [
        "### 10. Final Evaluation\n",
        "\n",
        "Finally, we load the best saved model and evaluate its performance on the unseen test set. We print a classification report and display a confusion matrix to analyze its performance on a per-class basis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW2Jv2bP-Fb9"
      },
      "outputs": [],
      "source": [
        "# Test evaluation\n",
        "print(\"Evaluating on test set...\")\n",
        "best_model_path = os.path.join(Config.MODEL_SAVE_DIR, 'best_model.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    model.load_state_dict(torch.load(best_model_path)['model_state_dict'])\n",
        "    test_loss, test_acc, test_predictions, test_labels = validate_epoch(\n",
        "        model, test_loader, criterion, device)\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(test_labels, test_predictions,\n",
        "                                target_names=list(Config.LABEL_MAPPING.keys())))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(test_labels, test_predictions)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=list(Config.LABEL_MAPPING.keys()),\n",
        "                yticklabels=list(Config.LABEL_MAPPING.keys()))\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(Config.RESULTS_DIR, 'confusion_matrix.png'))\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"No model found at {best_model_path}. Skipping evaluation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks55JPnj-Fb9"
      },
      "source": [
        "### 11. Save Final Results\n",
        "\n",
        "We save the final metrics and configuration to a JSON file for easy access and reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRz6cf6E-Fb9"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    'test_accuracy': test_acc if 'test_acc' in locals() else 'N/A',\n",
        "    'test_loss': test_loss if 'test_loss' in locals() else 'N/A',\n",
        "    'best_val_accuracy': best_val_acc,\n",
        "    'config': Config.__dict__(),\n",
        "    'label_mapping': Config.LABEL_MAPPING\n",
        "}\n",
        "\n",
        "results_path = os.path.join(Config.RESULTS_DIR, 'results.json')\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nTraining completed! Results saved in {Config.RESULTS_DIR}\")\n",
        "print(f\"Best model saved in {Config.MODEL_SAVE_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
